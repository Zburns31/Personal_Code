{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "import numpy as np  # http://www.numpy.org\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from math import log, floor, ceil\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the Utility class's methods. You can also add additional methods as required but don't change existing methods' arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Utility(object):\n",
    "    \n",
    "    # This method computes entropy for information gain\n",
    "    def entropy(self, class_y):\n",
    "        # Input:            \n",
    "        #   class_y         : list of class labels (0's and 1's)\n",
    "\n",
    "        # Compute the entropy for a list of classes\n",
    "        #\n",
    "        # Example:\n",
    "        #    entropy([0,0,0,1,1,1,1,1,1]) = 0.918 (rounded to three decimal places)\n",
    "\n",
    "        entropy = 0\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        base = 2\n",
    "        count = len(class_y)\n",
    "        # Get unique labels so we can find probability for each one\n",
    "        unique_labels = list(set(class_y))\n",
    "        \n",
    "        # Get probability for each class label\n",
    "        prob_vec = [class_y.count(label) / len(class_y) for label in unique_labels]\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = -sum([prob * log(prob, base) for prob in prob_vec])\n",
    "        #############################################\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def partition_classes(self, X, y, split_attribute, split_val):\n",
    "        # Inputs:\n",
    "        #   X               : data containing all attributes\n",
    "        #   y               : labels\n",
    "        #   split_attribute : column index of the attribute to split on\n",
    "        #   split_val       : a numerical value to divide the split_attribute\n",
    "\n",
    " \n",
    "\n",
    "        # Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "        # \n",
    "        # Split_val should be a numerical value\n",
    "        # For example, your split_val could be the mean of the values of split_attribute\n",
    "        #\n",
    "        # You can perform the partition in the following way\n",
    "        # Numeric Split Attribute:\n",
    "        #   Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "        #   the rows where the split attribute is less than or equal to the split value, and the \n",
    "        #   second list has all the rows where the split attribute is greater than the split \n",
    "        #   value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    " \n",
    "\n",
    "        '''\n",
    "        Example:\n",
    "\n",
    " \n",
    "\n",
    "        X = [[3, 10],                 y = [1,\n",
    "             [1, 22],                      1,\n",
    "             [2, 28],                      0,\n",
    "             [5, 32],                      0,\n",
    "             [4, 32]]                      1]\n",
    "\n",
    " \n",
    "\n",
    "        Here, columns 0 and 1 represent numeric attributes.\n",
    "\n",
    " \n",
    "\n",
    "        Consider the case where we call the function with split_attribute = 0 and split_val = 3 (mean of column 0)\n",
    "        Then we divide X into two lists - X_left, where column 0 is <= 3  and X_right, where column 0 is > 3.\n",
    "\n",
    " \n",
    "\n",
    "        X_left = [[3, 10],                 y_left = [1,\n",
    "                  [1, 22],                           1,\n",
    "                  [2, 28]]                           0]\n",
    "\n",
    " \n",
    "\n",
    "        X_right = [[5, 32],                y_right = [0,\n",
    "                   [4, 32]]                           1]\n",
    "\n",
    "        ''' \n",
    "\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "\n",
    "        y_left = []\n",
    "        y_right = []\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "\n",
    "        # Need to check for numeric or string types first\n",
    "        for i in range(len(X)):\n",
    "            # check if feature values are strings\n",
    "            if isinstance(X[i][split_attribute], str):\n",
    "                # Check if string value is equal, if it is, assign to left node\n",
    "                if X[i][split_attribute] == split_val:\n",
    "                    X_left.append(X[i])\n",
    "                    y_left.append(y[i])\n",
    "                else: # string value not equal, assign to right node\n",
    "                    X_right.append(X[i])\n",
    "                    y_right.append(y[i])\n",
    "                \n",
    "            else: # numeric features\n",
    "                if X[i][split_attribute] <= split_val:\n",
    "                    X_left.append(X[i])\n",
    "                    y_left.append(y[i])\n",
    "                else:\n",
    "                    X_right.append(X[i])\n",
    "                    y_right.append(y[i])\n",
    "        #############################################\n",
    "        return (X_left, X_right, y_left, y_right)\n",
    "\n",
    "\n",
    "    def information_gain(self, previous_y, current_y):\n",
    "        # Inputs:\n",
    "        #   previous_y: the distribution of original labels (0's and 1's)\n",
    "        #   current_y:  the distribution of labels after splitting based on a particular\n",
    "        #               split attribute and split value\n",
    "\n",
    "        # Compute and return the information gain from partitioning the previous_y labels\n",
    "        # into the current_y labels.\n",
    "        # You will need to use the entropy function above to compute information gain\n",
    "        # Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "\n",
    "        \"\"\"\n",
    "        Example:\n",
    "\n",
    "        previous_y = [0,0,0,1,1,1]\n",
    "        current_y = [[0,0], [1,1,1,0]]\n",
    "\n",
    "        info_gain = 0.45915\n",
    "        \"\"\"\n",
    "\n",
    "        info_gain = 0\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "\n",
    "        # Need to compare entropy of before splitting and the after for the left/right branches\n",
    "        entropy_before_split = self.entropy(previous_y)\n",
    "        entropy_left = self.entropy(current_y[0])\n",
    "        entropy_right = self.entropy(current_y[1])\n",
    "        \n",
    "        # Calculate number of total elements and the number of elements in each split\n",
    "        num_elements = len(list(itertools.chain.from_iterable(current_y)))\n",
    "        num_elements_left = len(current_y[0])\n",
    "        num_elements_right = len(current_y[1])\n",
    "        \n",
    "        # Calculate entropy using weighted average after the split has occured\n",
    "        entropy_split = ((num_elements_left / num_elements) * entropy_left + \n",
    "                         (num_elements_right / num_elements) * entropy_right)\n",
    "        \n",
    "        # Calculate information gain\n",
    "        info_gain = entropy_before_split - entropy_split\n",
    "        \n",
    "        #############################################\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        # Inputs:\n",
    "        #   X       : Data containing all attributes\n",
    "        #   y       : labels\n",
    "        #           : For each node find the best split criteria and return the split attribute, \n",
    "        #             spliting value along with  X_left, X_right, y_left, y_right (using partition_classes) \n",
    "        #             in the dictionary format {'split_attribute':split_attribute, 'split_val':split_val, \n",
    "        #             'X_left':X_left, 'X_right':X_right, 'y_left':y_left, 'y_right':y_right, 'info_gain':info_gain}\n",
    "        '''\n",
    "\n",
    "        Example: \n",
    "\n",
    "        X = [[3, 10],                 y = [1, \n",
    "             [1, 22],                      1, \n",
    "             [2, 28],                      0, \n",
    "             [5, 32],                      0, \n",
    "             [4, 32]]                      1] \n",
    "\n",
    "        Starting entropy: 0.971 \n",
    "\n",
    "        Calculate information gain at splits: (In this example, we are testing all values in an \n",
    "        attribute as a potential split value, but you can experiment with different values in your implementation) \n",
    "\n",
    "        feature 0:  -->    split_val = 1  -->  info_gain = 0.17 \n",
    "                           split_val = 2  -->  info_gain = 0.01997 \n",
    "                           split_val = 3  -->  info_gain = 0.01997 \n",
    "                           split_val = 4  -->  info_gain = 0.32 \n",
    "                           split_val = 5  -->  info_gain = 0 \n",
    "                           \n",
    "                           best info_gain = 0.32, best split_val = 4 \n",
    "\n",
    "\n",
    "        feature 1:  -->    split_val = 10  -->  info_gain = 0.17 \n",
    "                           split_val = 22  -->  info_gain = 0.41997 \n",
    "                           split_val = 28  -->  info_gain = 0.01997 \n",
    "                           split_val = 32  -->  info_gain = 0 \n",
    "\n",
    "                           best info_gain = 0.4199, best split_val = 22 \n",
    "\n",
    " \n",
    "       best_split_feature: 1  \n",
    "       best_split_val: 22  \n",
    "\n",
    "       'X_left': [[3, 10], [1, 22]]  \n",
    "       'X_right': [[2, 28],[5, 32], [4, 32]]  \n",
    "\n",
    "       'y_left': [1, 1]  \n",
    "       'y_right': [0, 0, 1] \n",
    "        '''\n",
    "        \n",
    "        split_attribute = 0\n",
    "        split_val = 0\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        max_info_gain = 0\n",
    "        num_features = len(X[0])\n",
    "        \n",
    "        for feature in range(num_features):\n",
    "            # Filter data to only include feature i\n",
    "            data = [i[feature] for i in X]\n",
    "            \n",
    "            for val in set(data):\n",
    "#             for val in range(0, max(data)):\n",
    "            \n",
    "                X_left, X_right, y_left, y_right = self.partition_classes(X, \n",
    "                                                                          y, \n",
    "                                                                          split_attribute = feature, \n",
    "                                                                          split_val = val\n",
    "                                                                         )\n",
    "                \n",
    "                info_gain = self.information_gain(y, [y_left, y_right])\n",
    "#                 print(f\"Info gain: {info_gain} for feature: {feature}, value: {val}\")\n",
    "#                 print(f\"Left split:{X_left} and right split: {X_right}\")\n",
    "                \n",
    "                if info_gain > max_info_gain:\n",
    "                    max_info_gain = info_gain\n",
    "                    best_split_attribute = feature\n",
    "                    best_split_val = val\n",
    "                    best_X_left = X_left\n",
    "                    best_X_right = X_right\n",
    "                    best_y_left = y_left\n",
    "                    best_y_right = y_right\n",
    "\n",
    "\n",
    "        return {'split_attribute':best_split_attribute, \n",
    "                'split_val':best_split_val, \n",
    "                'X_left':best_X_left, \n",
    "                'X_right':best_X_right, \n",
    "                'y_left':best_y_left,\n",
    "                'y_right':best_y_right, \n",
    "                'info_gain':max_info_gain\n",
    "               }\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classes 'DecisionTree' and 'RandomForest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please modify the 'DecisionTree' and 'RandomForest' classes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        self.tree = {}\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "\n",
    "    def learn(self, X, y, par_node = {}, depth=0):\n",
    "        # Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in Utility class to train the tree\n",
    "        \n",
    "        # par_node is a parameter that is useful to pass additional information to call \n",
    "        # the learn method recursively. Its not mandatory to use this parameter\n",
    "\n",
    "        # Use the function best_split in Utility class to get the best split and \n",
    "        # data corresponding to left and right child nodes\n",
    "        \n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a \n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        self.tree = self.construct_decision_tree(X, y, par_node = par_node, depth = depth)\n",
    "        \n",
    "    def construct_decision_tree(self, X, y, par_node, depth):\n",
    "        # Utility helper class to for calculating split points\n",
    "        utility = Utility()\n",
    "        \n",
    "        # base case 1: no data\n",
    "        if len(y) == 0:\n",
    "            return None\n",
    "\n",
    "        # base case 2: tree stops at previous level\n",
    "        if par_node is None:\n",
    "            return None\n",
    "        \n",
    "        # base case 3: if all labels are of one class\n",
    "        if all(x == y[0] for x in y):\n",
    "            return y[0]\n",
    "        \n",
    "        # base case 4: if max_depth is reached\n",
    "        if depth >= self.max_depth:\n",
    "            return self.get_majority(y)\n",
    "        \n",
    "        # base case 5: if 1 or 0 values remaining, we can't split on this\n",
    "        # Need to return \n",
    "        if len(y) <= 1:\n",
    "            return self.get_majority(y)\n",
    "        \n",
    "        if len(y) <= 1:\n",
    "            return self.get_majority(y)\n",
    "        \n",
    "        if len(X[0]) <= 1:\n",
    "            return self.get_majority(y)\n",
    "        \n",
    "        # Check to see if root node will perfectly segment classes\n",
    "        split_dict = utility.best_split(X, y)\n",
    "        \n",
    "        if len(split_dict[\"X_left\"]) == 0 or len(split_dict[\"X_right\"]) == 0:\n",
    "            return self.get_majority(y)\n",
    "        \n",
    "        else: # Tree construction\n",
    "            \n",
    "            # Set Root node of tree with split_attribute and split_value\n",
    "            par_node['Node'] = {\"split_attribute\": split_dict[\"split_attribute\"], \n",
    "                                \"split_val\": split_dict[\"split_val\"]\n",
    "                               }\n",
    "            \n",
    "            # Construct Left and Right subtrees\n",
    "            # Left tree: if val <= split_val\n",
    "            par_node[\"Left_Tree\"] = self.construct_decision_tree(X = split_dict[\"X_left\"], \n",
    "                                               y = split_dict[\"y_left\"],\n",
    "                                               par_node = {},\n",
    "                                               depth = depth + 1\n",
    "                                              )\n",
    "            # Right tree: if val > split_val\n",
    "            par_node[\"Right_Tree\"] = self.construct_decision_tree(X = split_dict[\"X_right\"], \n",
    "                                                y = split_dict[\"y_right\"],\n",
    "                                                par_node = {},\n",
    "                                                depth = depth + 1\n",
    "                                               )\n",
    "\n",
    "            return par_node\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    def get_majority(self, labels):\n",
    "        # return majority class label\n",
    "        return max(labels, key=labels.count)\n",
    "        \n",
    "        #############################################\n",
    "\n",
    "\n",
    "    def classify(self, record):\n",
    "        # classify the record using self.tree and return the predicted label\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        # Get current DT from instance attribute\n",
    "        node = self.tree\n",
    "\n",
    "        while isinstance(node, dict): # Iterate until we hit a left/right tree with an integer value\n",
    "            feat = node['Node']\n",
    "            split_attribute = feat[\"split_attribute\"]\n",
    "            split_val = feat[\"split_val\"]\n",
    "\n",
    "            if record[split_attribute] <= split_val:\n",
    "                node = node[\"Left_Tree\"]\n",
    "            else:\n",
    "                node = node[\"Right_Tree\"]\n",
    "\n",
    "        return node\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This starter code does not run. You will have to add your changes and\n",
    "# turn in code that runs properly.\n",
    "\n",
    "\"\"\"\n",
    "Here, \n",
    "1. X is assumed to be a matrix with n rows and d columns where n is the\n",
    "number of total records and d is the number of features of each record. \n",
    "2. y is assumed to be a vector of labels of length n.\n",
    "3. XX is similar to X, except that XX also contains the data label for each\n",
    "record.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class RandomForest(object):\n",
    "    num_trees = 0\n",
    "    decision_trees = []\n",
    "\n",
    "    # the bootstrapping datasets for trees\n",
    "    # bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "    bootstraps_datasets = []\n",
    "\n",
    "    # the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    # bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to records in\n",
    "    # the 'i'th bootstrapped dataset.\n",
    "    bootstraps_labels = []\n",
    "\n",
    "    def __init__(self, num_trees):\n",
    "        # Initialization done here\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [DecisionTree(max_depth=10) for i in range(num_trees)]\n",
    "        self.bootstraps_datasets = []\n",
    "        self.bootstraps_labels = []\n",
    "        \n",
    "    def _bootstrapping(self, XX, n):\n",
    "        # Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        #\n",
    "        # Create a sample dataset of size n by sampling with replacement\n",
    "        #       from the original dataset XX.\n",
    "        # Note that you would also need to record the corresponding class labels\n",
    "        # for the sampled records for training purposes.\n",
    "\n",
    "        sample = [] # sampled dataset\n",
    "        labels = []  # class labels for the sampled records\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "            \n",
    "        # Select n random indices from XX (dataset)\n",
    "        random_samples = np.random.choice(len(XX), size=n)\n",
    "        \n",
    "        # Remove labels from dataset which are located in the last position of the list\n",
    "        sample = [XX[i][:-1] for i in random_samples]\n",
    "        \n",
    "        # Get labels which are the last column in each sublist\n",
    "        labels = [XX[i][-1] for i in random_samples]\n",
    "        \n",
    "        #############################################\n",
    "        return (sample, labels)\n",
    "\n",
    "    def bootstrapping(self, XX):\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(XX, len(XX))\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "    def fitting(self):\n",
    "        # Train `num_trees` decision trees using the bootstraps datasets\n",
    "        # and labels by calling the learn function from your DecisionTree class.\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        \n",
    "        for i in range(len(self.decision_trees)):\n",
    "            print(f\"Building Decision Tree {i}\")\n",
    "            self.decision_trees[i].learn(self.bootstraps_datasets[i], self.bootstraps_labels[i])\n",
    "            \n",
    "        #############################################\n",
    "\n",
    "    def voting(self, X):\n",
    "        y = np.array([], dtype = int)\n",
    "\n",
    "        for record in X:\n",
    "            # Following steps have been performed here:\n",
    "            #   1. Find the set of trees that consider the record as an\n",
    "            #      out-of-bag sample.\n",
    "            #   2. Predict the label using each of the above found trees.\n",
    "            #   3. Use majority vote to find the final label for this recod.\n",
    "            votes = []\n",
    "            \n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "                \n",
    "                if record not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify(record)\n",
    "                    votes.append(effective_vote)\n",
    "\n",
    "            counts = np.bincount(votes)\n",
    "\n",
    "            dt_votes = []\n",
    "            if len(counts) == 0:\n",
    "                # Special case\n",
    "                #  Handle the case where the record is not an out-of-bag sample\n",
    "                #  for any of the trees.\n",
    "                #############################################\n",
    "                # If record used by all DT's in sampling, lets use the majority\n",
    "                print(\"Special Case: Using majority vote across DT's\")\n",
    "\n",
    "                # Loop through each decision tree and get the prediction and store it\n",
    "                # so we can take the majority decision\n",
    "                for tree in self.decision_trees:\n",
    "                    dt_votes.append(tree.classify(record))\n",
    "                \n",
    "                # get counts for each class and take the maximum value\n",
    "                vote_counts = np.bincount(dt_votes)\n",
    "                y = np.append(y, np.argmax(vote_counts))\n",
    "                \n",
    "                #############################################\n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "                \n",
    "        return y\n",
    "\n",
    "    def user(self):\n",
    "        \"\"\"\n",
    "        :return: string\n",
    "        your GTUsername, NOT your 9-Digit GTId  \n",
    "        \"\"\"\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        return 'zburns6'\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# TODO: Determine the forest size according to your implementation. \n",
    "# This function will be used by the autograder to set your forest size during testing\n",
    "# VERY IMPORTANT: Minimum forest_size should be 10\n",
    "def get_forest_size():\n",
    "    forest_size = 10\n",
    "    return forest_size\n",
    "\n",
    "# TODO: Determine random seed to set for reproducibility\n",
    "# This function will be used by the autograder to set the random seed to obtain the same results you achieve locally\n",
    "def get_random_seed():\n",
    "    random_seed = 42\n",
    "    return random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    np.random.seed(get_random_seed())\n",
    "    # start time \n",
    "    start = datetime.now()\n",
    "    X = list()\n",
    "    y = list()\n",
    "    XX = list()  # Contains data features and data labels\n",
    "    numerical_cols = set([i for i in range(0, 9)])  # indices of numeric attributes (columns)\n",
    "\n",
    "    # Loading data set\n",
    "    print(\"reading the data\")\n",
    "    with open(\"pima-indians-diabetes.csv\") as f:\n",
    "        next(f, None)\n",
    "        for line in csv.reader(f, delimiter=\",\"):\n",
    "            xline = []\n",
    "            for i in range(len(line)):\n",
    "                if i in numerical_cols:\n",
    "                    xline.append(ast.literal_eval(line[i]))\n",
    "                else:\n",
    "                    xline.append(line[i])\n",
    "\n",
    "            X.append(xline[:-1])\n",
    "            y.append(xline[-1])\n",
    "            XX.append(xline[:])\n",
    "\n",
    "    # Initializing a random forest.\n",
    "    randomForest = RandomForest(get_forest_size())\n",
    "\n",
    "    # printing the name\n",
    "    print(\"__Name: \" + randomForest.user()+\"__\")\n",
    "\n",
    "    # Creating the bootstrapping datasets\n",
    "    print(\"creating the bootstrap datasets\")\n",
    "    randomForest.bootstrapping(XX)\n",
    "\n",
    "    # Building trees in the forest\n",
    "    print(\"fitting the forest\")\n",
    "    randomForest.fitting()\n",
    "\n",
    "    # Calculating an unbiased error estimation of the random forest\n",
    "    # based on out-of-bag (OOB) error estimate.\n",
    "    y_predicted = randomForest.voting(X)\n",
    "\n",
    "    # Comparing predicted and true labels\n",
    "    results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "    print(\"accuracy: %.4f\" % accuracy)\n",
    "    print(\"OOB estimate: %.4f\" % (1 - accuracy))\n",
    "\n",
    "    # end time\n",
    "    print(\"Execution time: \" + str(datetime.now() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data\n",
      "__Name: zburns6__\n",
      "creating the bootstrap datasets\n",
      "fitting the forest\n",
      "Building Decision Tree 0\n",
      "Building Decision Tree 1\n",
      "Building Decision Tree 2\n",
      "Building Decision Tree 3\n",
      "Building Decision Tree 4\n",
      "Building Decision Tree 5\n",
      "Building Decision Tree 6\n",
      "Building Decision Tree 7\n",
      "Building Decision Tree 8\n",
      "Building Decision Tree 9\n",
      "Special Case: Using majority vote across DT's\n",
      "Special Case: Using majority vote across DT's\n",
      "Special Case: Using majority vote across DT's\n",
      "Special Case: Using majority vote across DT's\n",
      "Special Case: Using majority vote across DT's\n",
      "Special Case: Using majority vote across DT's\n",
      "Special Case: Using majority vote across DT's\n",
      "accuracy: 0.8451\n",
      "OOB estimate: 0.1549\n",
      "Execution time: 0:00:10.445556\n"
     ]
    }
   ],
   "source": [
    "# Call the run() function to test your implementation\n",
    "# Use this cell and any cells below for additional testing\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}